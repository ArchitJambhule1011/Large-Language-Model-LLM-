{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.6325, -4.2024, -2.0181], grad_fn=<SqueezeBackward3>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA linear transformation, in the context of neural networks and linear algebra, \\nis a mathematical operation that takes an input vector (or tensor) and produces \\nan output vector through a linear mapping\\n\\nthe specific linear transformation is represented by the nn.Linear(3, 3, bias=False) line, where:\\n\\n    3 represents the dimensionality of the input vector.\\n    3 represents the dimensionality of the output vector.\\n    bias=False indicates that there are no bias terms associated with this linear transformation.\\n\\n\\nMathematically, a linear transformation can be represented as follows:\\ny = Wx\\nWhere:\\n    y is the output vector.\\n    x is the input vector.\\n    W is the weight matrix, which defines how the input is transformed to produce the output.\\n\\nthe nn.Linear module initializes this weight matrix W based on the specified input and output dimensions. \\nWhen you apply this linear transformation to the sample tensor with linear(sample), it calculates the \\ndot product between the input vector sample and the weight matrix W, producing a 3-dimensional output vector.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.tensor([10.,10.,10.])\n",
    "linear = nn.Linear(3,3, bias=False)\n",
    "print(linear(sample))\n",
    "\n",
    "'''\n",
    "A linear transformation, in the context of neural networks and linear algebra, \n",
    "is a mathematical operation that takes an input vector (or tensor) and produces \n",
    "an output vector through a linear mapping\n",
    "\n",
    "the specific linear transformation is represented by the nn.Linear(3, 3, bias=False) line, where:\n",
    "\n",
    "    3 represents the dimensionality of the input vector.\n",
    "    3 represents the dimensionality of the output vector.\n",
    "    bias=False indicates that there are no bias terms associated with this linear transformation.\n",
    "\n",
    "\n",
    "Mathematically, a linear transformation can be represented as follows:\n",
    "y = Wx\n",
    "Where:\n",
    "    y is the output vector.\n",
    "    x is the input vector.\n",
    "    W is the weight matrix, which defines how the input is transformed to produce the output.\n",
    "\n",
    "the nn.Linear module initializes this weight matrix W based on the specified input and output dimensions. \n",
    "When you apply this linear transformation to the sample tensor with linear(sample), it calculates the \n",
    "dot product between the input vector sample and the weight matrix W, producing a 3-dimensional output vector.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tensor_1 = torch.tensor([1.0, 2.0,3.0])\n",
    "softmax_out = F.softmax(tensor_1, dim=0)\n",
    "\n",
    "print(softmax_out)\n",
    "\n",
    "''' \n",
    "The softmax function first exponentiates each element of the input tensor. \n",
    "In our case, we calculate e^1.0, e^2.0, and e^3.0.\n",
    "\n",
    "These exponentiated values are then normalized. \n",
    "This involves dividing each exponentiated value by the sum of all the exponentiated values.\n",
    "\n",
    "softmax_out[0] = e^1.0 / (e^1.0 + e^2.0 + e^3.0)\n",
    "softmax_out[1] = e^2.0 / (e^1.0 + e^2.0 + e^3.0)\n",
    "softmax_out[2] = e^3.0 / (e^1.0 + e^2.0 + e^3.0)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding example\n",
    "\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "input_indices = torch.LongTensor([1,5,3,2])\n",
    "\n",
    "embedded_output = embedding(input_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7141,  0.0562, -0.2028, -0.7535, -2.0878,  1.8895,  0.4802, -1.3629,\n",
       "        -0.6780, -0.5944,  0.0061, -2.4262, -0.1263, -1.6117,  0.0049, -1.7066,\n",
       "         0.2956,  0.4144,  0.5867,  0.7812,  0.2897, -0.4531,  2.2317, -0.6937,\n",
       "        -0.3222,  0.3711,  0.6082, -0.4153,  1.2835, -1.6107,  0.9861,  1.2927,\n",
       "        -1.3491, -0.1391, -0.7601,  1.1158, -0.2805, -1.2785, -0.6942, -1.3419,\n",
       "        -0.6739,  1.1143, -2.4807, -0.4945, -1.5342, -0.2980, -1.6055,  0.7699,\n",
       "         0.4636, -0.0799, -0.2189,  0.0073,  0.8336, -0.8915, -1.7746, -0.5397,\n",
       "        -0.5808,  0.0962,  1.9988, -0.8394,  1.6788, -1.0648,  0.0757, -0.8841,\n",
       "         0.2622, -0.9124, -1.0700, -0.0353,  0.1188,  0.8628, -0.4179, -0.9719,\n",
       "         0.8884,  1.1437, -0.1747, -0.7622,  0.2384, -0.5126,  0.6451, -0.3315,\n",
       "        -0.8950,  0.7832,  0.4548, -0.0414, -0.9836,  0.3304,  0.7972, -1.6520,\n",
       "         0.3203, -0.3643,  1.6726, -1.0870,  0.4368, -0.3060,  0.4171, -0.8036,\n",
       "         1.1157, -0.8875,  0.3075,  0.7693], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2, 3])\n",
      "result tensor([[ 22.,  30.,  28.],\n",
      "        [ 46.,  68.,  60.],\n",
      "        [ 70., 106.,  92.]])\n"
     ]
    }
   ],
   "source": [
    "# Dot products and matmuls\n",
    "\n",
    "a = np.array([[1,2],[3,4],[5,6]])\n",
    "b = np.array([[2,8,4],[10,11,12]])\n",
    "\n",
    "a_tensor = torch.Tensor(a)\n",
    "b_tensor = torch.Tensor(b)\n",
    "\n",
    "print(a_tensor.shape)\n",
    "print(b_tensor.shape)\n",
    "\n",
    "print('result', a_tensor @ b_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Logits and view\n",
    "\n",
    "a = torch.rand(2,3,5)\n",
    "x, y , z = a.shape\n",
    "\n",
    "a = a.view(x,y,z)\n",
    "print(a.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
